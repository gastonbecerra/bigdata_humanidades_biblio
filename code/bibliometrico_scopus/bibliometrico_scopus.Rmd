---
title: "Social sciences and humanities on big data: a bibliometric analysis"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  comment = '', fig.width = 8, fig.height = 3
  )

options(scipen = 999)

library(tidyverse)
library(igraph)
library(ggraph)
library(tidygraph)
library(gt)

metadata <- readr::read_csv(file = "../../data/metadata.csv")
ocr <- readr::read_csv(file = "../../data/ocr.csv")
autores <- readr::read_csv(file = "../../data/autores.csv")
references <- readRDS(file = "../../data/referencias/referencias.rds")

```

***Purpose**: In recent years, the rapid growth of big data has presented immense potential for business applications as well as raised great interest from academia. In response to this emerging phenomenon, the purpose of this paper is to provide a comprehensive literature review of big data.*
***Design/methodology/approach**: A bibliometric method was used to analyze the articles obtained from the Scopus database published between 2013 and 2018. A sample size of 4,070 articles was evaluated using SciVal metrics.* 
***Findings**: The analysis revealed an array of interesting findings as follows: 
- the number of publications related to big data increased steadily over the past six years, though the rate of increase has slowed since 2014; 
- the scope of big data research is quite broad in regards to both research domains and countries; 
- despite a large volume of publications, the overall performance of big data research is not well presented as measured by the field-weighted citation impact metric; - collaboration between different institutions, particularly in the form of international collaboration and academic–corporate collaboration, has played an important role in improving the performance of big data research.
- over 50% of publications do not receive any citations, and the average number of citations per publication is 3.17. 
- It is also observed that single authorship of research publications has declined over the time. 
- The analysis reveals the pioneering role played by the USA in advancing the research in big data, which has lately been taken over by China, and the large-scale usage of big data analytics in various domains of science.*
***Originality/value**: To the best of the authors’ knowledge, this is the first study to provide a holistic view of the big data research. The insights obtained from the analysis are instrumental for both academics and practitioners.* **FALTA EL ABSTRACT**

**Keywords**: Big data, social sciences, humanities, bibliometric analysis, citation analysis


# Introduction


This work explores, through a bibliometric analysis, the scientific literature about "Big data" in social sciences, psychology and humanities. Working on a 5,500 papers dataset retrieved from Scopus, we aim to identify trends about authorship and collaboration, topics and study areas, and the most influential works. Such objective is a particular step within a broader project aiming to compare how big data is thematized and framed in different social systems, such as mass media, science, politics, commerce, economics, among others (Becerra, 2018). 

The term "big data" originated in late 1990s in the IT sector, referring to the (technical) challenges of handling a vast amount of information (Diebold, XXXX). In a famous consultancy piece, Douglas Laney (2001) synthesized these challenges by referencing 3 v’s -volume, velocity, and variety-, a formula that expanded to include other v-words such as, visualization or value, and that usually takes the place of a definition. Ironizing this, "Vexatious vagueness" is the implicit last v-word, says Halavais (2015).

From here, big data has expanded to different areas of social life, such as politics, mass media, business, among others. Critical case studies, from a social science perspective, are still required to understand the social meaning of big data in such spaces. 

Drawing mostly on mass media discourse, in another work (Becerra, 2021) we've proposed that communications regarding big data usually include 2 highly debatable elements: a premise regarding the availability of huge volumes of data that can be exploited; and a promise that new knowledge and understandings about all aspects of human life will be reached through data analysis. These elements are part of a widespread belief, rhetoric and mythology that relate big data with other socio-technical developments, such as artificial intelligence and algorithms, and offer the promise of an objective, effective and optimal, value-free, and conflict-free social future (boyd & Crawford, 2021; van Dijck, 2014; Sadin, 2018; among others).

Scientific interest on big data has also been on the rise in the last decade. According to several studies (e.g., Belmonte, et. al., 2020; Liu et. al., 2019), and drawing on different sources, the scientific big data literature has grown yearly in a X2 rate for the 2010-2014 range; and, although this trend has slowed down, up until 2020 the number of papers per year never decreased. Others have shown that the estimated contribution from the social sciences and humanities is near the 7-10% of this corpus (Kalantari et. al., 2017; Liu et. al., 2019). 

Facing big data, social sciences and humanities found a big challenge: to criticize and discuss the premise and the promise of its rhetoric and mythology, to disect the social beliefs and ideologies around it, to illuminate the extent of social and ethical issues that it brings, and to re-shape it from within to advance our understanding of ourselves. 

To dwell on a few of these: 

* philosophy and other humanities have been discussing the harms of research on big social data -e.g., data collection through Facebook apps and active campaigns-, pinpointing issues on privacy and social reactivity in a time when the research/experimentation seems to be fading and regulatory definitions demand revisions (Metcalf & Crafword, 2016; Mai, 2016; Leonelli, 2016; Weinhardt, 2020; Zuboff, 2015); 
* social sciences and epistemology have warned that big social data, and any other sources generated in the datafication process, are embedded with limitations and conditioning from the data gathering/creation settings, and that also the data curation process required for algorithmical analysis is a heavily decision task that challenge the neutrality and objectivity of some data science claims (Mutzel, 2015; Gitelman, 2013; Qiu et. al., 2018)
* also social sciences, psychology and humanities have been discussing different integrations big data in social research, in a way that could allow to guide data-driven analysis through the rich theoretical awareness of these disciplines (Frade, 2016; Halford & Savage, 2017; Burrows & Savage, 2014; boyd & Crawford, 2012). As Halavais (2015) synthezises with regards to sociology:

> "Big data does provide a challenge to the social sciences, but not a particularly new one. It is, in fact, the core challenge of sociology: connecting the micro-connections between individuals to the vast social structures that shape us (and are shaped by us) as a society. Mills famously suggested that this ability to both connect and disconnect the personal with the social was at the core of what he called the ‘sociological imagination’."

In this work we focus on literature from social sciences, psychology and humanities & arts. We do so in a bibliometric approach that aims at assessing and analyzing in a quantitative manner trends in the publications. Specifically, we are interested in 3 key issues for which bibliometric analysis has proven to be an useful and a valid methodology: (1) Authorship: access to big data is unequal because of costs and skills. This is true for the users and citizens but also within the academic world (McCarthy, 2016). Analyzing authorship is a way to shed some light on these issues. Also, although using co-authorship as proxy for real academic collaboration has been discussed (Ponomariov & Boardman, 2016), it is indeed and interesting metric to explore trends in integration and crossings between nations and types of institutions (e.g., academia and private sector), which are, argueably, requirements of big data research; (2) Contents: big data is a trans-disciplinary area of research with no clear taxonomy nor classifications. Analyzing content in a statistical manner can help us gain insights about the different ways in which it is framed and thematized in the different branches of knowledge; (3) Citations: although citation should not be taken as a valid tool to assess the quality of a publication, it is indeed a valuable metric to assess visibility and influence (Eibrahim, 2014). 

Our research questions are:

* **RQ1** What is the authorship and collaboration trend?
  * **RQ1.1** Which countries and institutions have contributed most in terms of paper count? What is the collaboration between countries, and between type of institution like?
  * **RQ1.2** What is the average number of authors per publications? What is the authorship and collaboration trend?
  
* **RQ2** What are the key topics/areas that are addressed in publications? 

  
* **RQ3** Citaciones
  * **RQ3.1** What are the top cited publications and venues?
  * **RQ3.2** 
  * **RQ3.3** cuantas citas por año?
  * **XXXXX  a que clasicos se cita? (para otro laburo!)**

**falta aclarar secciones**


# Related works


Several papers have used bibliometric analysis to map the big data scientific production, with different goals and levels of analysis. The closest to our interests, and the most updated and comprehensive study, is Ahmad et. al. (2020). They analyzed 33,623 works from 2008 to 2017, retrieved via Scopus, by searching "big data" in title and keywords. They did not limit the literature search to computer science alone, but allowed results from social, medical and business sciences. In fact, the only limit reported is English as language, and the exclusion of conference review, editorial notes, and letters document types. Their main objective was to spot top cited papers, the contributions of countries, and to identify key research areas.

Drawing also on Scopus, Liu et. al. (2019) conducted a bibliometric analysis in a set of 4,070 articles published between 2013 and 2018. Besides analyzing trends in production and collaboration across countries and institutions, they used Scopus' SciVal to evaluate the performance of the papers and calculated field-weighted citation impact. 

Kalantari et. al. (2017) analyzed 6,572 papers from the Thomson Reuters’ Web of Science (WoS) core collection database between 1980 and 2015. Interestingly, they worked with a wide range of search keywords gathered by experts (arguibly from computer sciences) interviews and survey. In addition to general trends about production by year, country and research area, and about ranking of journals and authors, plus a comprehensive citation analysis, they provided a multi-regression analysis in order to explore how number of pages, references, and authors contibuted to the citation received. 

Drawing on 5,840 articles retrieved Web of Science (WoS) covering the period between 2000 and 2015, Zhang et. al. (2019) performed bibliometric analysis and text-mining techniques, to show time and geographic distribution and core constituents, and also to simulate, visualize and predict the evolution of topics -mostly about technological changes- related to big data. 

Belmonte et. al. (2020) review 4,240 papers related to big data and machine learning for the period 2010-2019, indexed in Web of Science (WoS). Their focus was on performance of these works, using bibliometric indicators such as h-index, g, and others, in a dynamic view that tried to analyze the path and projection of both terms. The authors concluded that machine learning is the topic with the highest bibliometric levels, and the one that most probably will be driving the research enterprise, even above big data.

Finally, Liang and Liu (2018) analyzed 10,637 publications associated with big data and 1,168 publications associated with business intelligence, retrieved from Social Science Citation Index, Science Citation Index Expanded and Arts & Humanities Citation Index for 1990 to 2017. They performed a dynamical analysis to find trends about emerging topics and explore their disciplinary distribution. Further, they subset their corpus, and focused on 2,819 papers from social sciences and humanities journals to explore topics and citations. To the best of our knowledge this is the only work that has focused, although briefly and partially, on the disciplines we are interested in.

Besides these general reviews, there are several studies that focus on specific areas and topics related to big data. 
E.g., Both Mishra et. al. (2018) and Inamdar et. al. (2020) performed bibliometrics and in-depth literature review concerning big data and big data analitycs adoption for supply chain management and related areas, with datasets of 286 articles published in 2006-2016 and retrieved via Scopus, and 79 papers published between 2014-2018 retrieved via Web of Science (WoS); 
Rialti et. al. (2019) systemathized 170 articles from the Clarivate Analytics Web of Science Core Collection database regarding big data and (managerial) dynamic capabilites, in order to provide and in-depth literature review. 
Other bibliometrics and text-mining analysis were performed for business publications (Amado, Cortez, Rita, & Moro, 2018; Moro, Cortez, & Rita, 2015; Ngai, Xiu, & Chau, 2009). Finally, although is not a bibliometric work, De Mauro et. al. (2015) presented a very interesting topic analysis on big data publications that resulted in a conventional definition of this phenomena in the academia and business. 

In the Results section we compare and synthesize these works regarding authorship, topics and citations, in order to provide a context or benchmark for our results.

# Methodology

We selected the database Scopus to construct our corpus because it offered both complete metadata, abstracts, and references for the articles. Others databases, such as Jstor and Ebsco, were also explored but, although full-text content was available in some cases, the number of articles was far less, and lacked of several abstracts and references. 

For the search we used the criteria "big data" in title, abstracts, and keywords. We limited the subject area to social sciences, psychology and humanities and arts, the language to English, and the publication type to journal articles. The dataset was exported the first of December of 2020. Although initially no date limits were applied, most of the publications were comprised between 2010-2020. From the original result of 5,610 records, we kept 5,500 that were included in this shorter timespan, and had abstracts. 

```{r prepro, echo=FALSE, message=FALSE, warning=FALSE}

keywords <- metadata %>% filter(tag=="keyword")
articles <- metadata %>%
  select(-source) %>% 
  filter(!tag == "keyword") %>%
  pivot_wider(id_cols = id, names_from = tag, values_from = value) %>%
  left_join(keywords %>% count(id) %>% rename(keywords=n)) %>%
  mutate(
    year=as.integer(year),
    cited=as.integer(cited)
    ) %>%
  filter(
    year>=2010,
    !is.na(abstract)
    )

glimpse(articles)

```

Keywords, abstracts and other metadata had only minimal pre-processing (text transformed to lowercase, removed symbols and numbers). References were parsed using the RubyGem version of Anystyle.[^1] All the analysis and visualization were done with statistical software R (Team R Core, 2018).

[^1]: https://anystyle.io/

Most of our research questions did not required more than descriptive statistics and visualization techniques. To express the author's collaboration we used Subramanyam Index (Subramanyam, 1983) that calculates the proportion of articles with +2 authors over the total of articles. Institutional affiliation required some parsing to identify it as "academic" or "corporation". We used "universi|college|school|institu|academ|politec" search criteria for this purpose. To identify topics and key issues we performed topic modeling over the abstracts, using  the Latent Dirichlet Allocation model (D. Blei, Ng, & Jordan, 2003), via the XXXXX R package (Grün & Hornik, 2011), which posits different distributions of the corpus’ vocabulary as topics, and calculates the proportional mix of them for each document. Since the number of topics must be introduced as a parameter, after several runs and statistical tests with different parameters, we settled on a X topics solution. By theoretically interpreting representative documents for each of the topic detected, we formulated a taxonomy of X categories. **ESTA ULTIMA ORACION ES UNA GAROMPA**


# Results

According to the surveys, big data literature has grown yearly in over a X2 rate for the 2010-2014 range, then this trend has slowed down near 2015, and up 2020 the number of papers per year never decreased (e.g., Belmonte, et. al., 2020; Liu et. al., 2019). Our dataset follows this general trend.

```{r tiempo, echo=FALSE, message=FALSE, warning=FALSE}

articles %>% count(year) %>% 
  mutate(source="Own elaboration") %>%
  ggplot(aes(x=year,y=n)) + # , color=source
  geom_line() + 
  geom_point() +
  geom_label(aes(label=n)) +
  theme_minimal() +
  scale_x_continuous(name="Years", 
                     breaks = c(2010:2020), 
                     labels = c(2010:2020),
                     minor_breaks = NULL) +
  scale_y_continuous(name="Number of Publications") + 
  labs(caption="Figure 1. Evolution of scientific production in soc. sci. & humanities") + 
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

ir_own <- articles %>% count(year) %>% pull(n)
names(ir_own) <- 2010:2020

ir_liu <- c("2013"=101, "2014"=208, "2015"=555, "2016"=809, "2017"=1052, "2018"=1273)
ir_belmonte <- c("2010"=1, "2011"=1, "2012"=15, "2013"=58, "2014"=176, "2015"=352, "2016"=547, "2017"=785, "2018"=1152, "2019"=1166)

increase_rate <- function(v) {
  return(( v - lag(x = v, n = 1) ) / lag(x = v, n = 1) *100)
}

l=list(increase_rate(ir_own),increase_rate(ir_liu),increase_rate(ir_belmonte))
ir <- do.call(rbind, lapply(l, function(x) x[match(names(l[[1]]), names(x))])) %>% as.data.frame()
rownames(ir) = c("Own elaboration","Liu, et. al., 2019","Belmonte, et. al., 2020")

ir %>% rownames_to_column() %>% 
  gt() %>% 
  fmt_percent(columns = everything(), decimals = 2, scale_values = FALSE) %>%
  fmt_missing(columns = everything(), missing_text = "-") %>%
  tab_header(
    title = "% Yearly increase of scientific production in different corpora"
  ) %>%
  tab_options(
    table.font.size = "11px"
  ) 

rm(ir_own,ir_liu,ir_belmonte,l,increase_rate,ir)

```


## RQ1. Authorship and collaboration trends


According to the general big data literature, in terms of production by countries,  People’s Republic of China leads the ranking, followed by USA, sharing near 50% of the different corpora. USA was leading the production up until 2015 approx, when China production exploded. These countries appear in inverse order only in Kalatari et. al. (2017) and Belmonte et. al. (2020), which includes a much larger and earlier timespan considered, and a broader set of search criteria. Following these are India, UK, France, Germany, South Korea, in different orders depending on the corpus. USA and China are the countries that collaborate the most, both between them and with other countries. Comparatively, Canada, Autralia, Switzerland and Japan show less production than the other aforementioned countries, but have a larger centrality in collaborations. South American, Middle-eastern, and African countries show the least level of production. 

Our results[^2] shows USA on top of the ranking for scientific production regarding big data in the social sciences and humanities. China is in second place, although its production spiked up after 2018, surpassing USA in 2020. 

[^2]: Following Kalantari et. al. (2017) we sum each individual author's affiliation and country to achieve this result.

```{r paises, echo=FALSE, fig.height=5, fig.width=8, message=FALSE, warning=FALSE}

autores %>% 
  left_join( articles ) %>%
  filter(!is.na(pais), year>=2010) %>% 
  count(pais, year, sort = TRUE) %>% 
  group_by(pais) %>% mutate(
    total_papers=sum(n),
    anios=n()
    ) %>% ungroup() %>%
    mutate(pais=paste(str_to_title(pais), " (n=", total_papers, ")", sep = "")) %>%
    filter(total_papers>50) %>%
    ggplot(aes(x=year,y=reorder(pais,total_papers),fill=n)) + 
      geom_tile() +
      geom_text(aes(label=n), color="white", show.legend = FALSE, size=3) +
      theme_minimal() +
      scale_x_continuous(name="Years", 
                     breaks = c(2010:2020), 
                     labels = c(2010:2020),
                     minor_breaks = NULL) +
  scale_fill_gradient(low="lightblue", high="red") +
  labs(caption="Figure 2. Production by country and year (countries with n > 50)") +
  theme(axis.title.y=element_blank()) +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

```

Going back to the general big data research literature, iInstitutional affiliation has been analyzed in far less extent. Ahmad et. al. (2020) reports that Chinese institutions, such as Chinese Academy of Sciences and Tsinghua University, not only rank on top of the production but also in the concentration of papers per institution; USA production seems to be less concentrated. Again, Belmonte et. al. (2020) registered and inverse ranking, with Harvard and universities from California and Texas on top. Zhang et. al. (2019) highlights that world-class universities from USA, although not that prominent as Chinese institutions, are much more interconnected in terms of collaboration. Liu et. al. (2019) reported that only 2.8% of the papers of its dataset are produced jointly between academia and corporations. The authors conclude that "this finding is likely to indicate that, in the field of big data research, joint work contributed by both academia and industry is not popular". The corporate institutions that collaborate the most with academia are IBM, Intel and Microsoft.

If we analyze our dataset, the collaboration between academia and corporate (Fig. 3) seems to be growing slowly within social sciences and humanities. Top institutions, in terms of affiliation of authors (Fig. 4), are the same as registered in the general big data field, although in different order, reflecting the dominance of USA.

```{r acc, echo=FALSE, message=FALSE, warning=FALSE}

academicAff <- "universi|college|school|institu|academ|politec"

cleanAff <- function(txt) {
  z <- c()
  for(i in 1:length(txt)){
    if (!is.na(txt)){
      txt2 <- str_split(string = txt[i], pattern = ",", simplify = TRUE) %>%
        as.character()
      txt3 <- grepl(
          pattern = academicAff, 
          txt2, ignore.case = TRUE)
      if(TRUE %in% txt3) {
        z <- c(z, trimws(txt2[max(which(txt3==TRUE))]))  
      } else {
        z <- c(z, trimws(txt2[1]))
      }
    } else {
      z <- c(z, NA)
    }
  } 
  return(z)  
}

autores2 <- autores %>% 
  left_join(articles %>% select(id,year)) %>% filter(year>=2010) %>%
  filter(!is.na(aff)) %>%
  mutate(
    uni = grepl(
      pattern = academicAff, 
      aff, ignore.case = TRUE),
    aff2 = cleanAff(aff)
    ) 

acc <- autores2 %>% count(id,uni) %>% 
  pivot_wider(names_from = uni, values_from = n) %>%
  rename(academy=2,private=3) %>%
  mutate( acc=case_when(
    !is.na(academy) & !is.na(private) ~ "ACC",
    !is.na(academy) & is.na(private) ~ "Only academia",
    is.na(academy) & !is.na(private) ~ "Only corporate"
    )
  )

acc %>% 
  left_join(articles %>% select(id,year)) %>% filter(year>=2010) %>%
  count(acc, year) %>%  # cuantos puros y cuantos acc
  ggplot(aes(x=year,y=n,fill=acc)) +
    geom_col(position = "fill", stat = "identity") +
    labs(fill = "Autorship Type")+
  theme_minimal() +
      scale_x_continuous(name="Years", 
                     breaks = c(2010:2020), 
                     labels = c(2010:2020),
                     minor_breaks = NULL) +
  labs(caption="Figure 3. Distribution of autorship type") +
  theme(axis.title.y=element_blank()) +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

autores2 %>% 
  mutate(inst_type=case_when(
    uni ~ "Academia",
    !uni ~ "Corporate"
    )) %>%
  left_join(articles %>% select(id,year)) %>% filter(year>=2010) %>%
  count(inst_type,aff2) %>% 
  group_by(inst_type) %>% 
  mutate(aff2=str_to_title(str_sub(aff2,1,30))) %>%
  slice_max(n = 10, order_by=n) %>%
  ggplot(aes(y=n,x=reorder(aff2,n))) + geom_col() +
  coord_flip() +
  facet_wrap(~inst_type, scales = "free")+
  theme(axis.title.x=element_blank(), 
        axis.title.y=element_blank()) +    
  theme_minimal() +
  theme(legend.position = "none")+
  labs(caption="Figure 4. Top 10 affiliations by number of papers, in academia and corporate") +
  theme(axis.title.y=element_blank()) +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

rm(acc, academicAff, cleanAff, autores2)

```

### Colaboración entre paises

```{r colaboracion-mapa}


# library(ggmap)
# Sys.setenv(google_key = "AIzaSyAGSGP7CUWpxDJkxjVe_UKGaV0U1Ea0gn0")
# register_google(key = Sys.getenv("google_key"))
# pp <- V(paises_corr_graph) %>% as.character()
# geoloc <- ggmap::geocode(location = pp)
# geoloc$pais <- pp
# geoloc %>% write.csv("../data/geoloc.csv")

# https://datascience.blog.wzb.eu/2018/05/31/three-ways-of-visualizing-a-graph-on-a-map/
# https://rstudio-pubs-static.s3.amazonaws.com/339257_fd4db2366a1f4e44ae95a0fd6db24457.html
# http://dyerlab.github.io/popgraph/

paises <- autores %>% 
  filter(!is.na(pais)) %>%
  count(pais, sort = TRUE)

glimpse(paises)

library(rgeos)
library(rworldmap)

wmap <- getMap(resolution="high")
centroids <- gCentroid(wmap, byid=TRUE)
paises2 <- as.data.frame(centroids)
map_world <- map_data(map = "world") %>%
  filter(region != "Antarctica")

maptheme <- theme(panel.grid = element_blank()) +
  theme(axis.text = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(axis.title = element_blank()) +
  theme(legend.position = "bottom") +
  theme(panel.grid = element_blank()) +
  theme(panel.background = element_rect(fill = "#596673")) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'cm'))

country_shapes <- geom_polygon(aes(x = long, y = lat, group = group),
                               data = map_data('world'),
                               fill = "#c9c9c9", color = "#515151",
                               size = 0.15)
mapcoords <- coord_fixed(xlim = c(-150, 180), ylim = c(-55, 80))





paises_corr <- autores %>% 
  filter(!is.na(pais)) %>%
  group_by(pais) %>% filter(n() > 50) %>% ungroup() %>%  
  widyr::pairwise_count(item = pais, feature = id, sort = TRUE) 

paises_corr_graph <- tidygraph::as_tbl_graph(
  paises_corr,
  directed = FALSE) %>%
  activate(nodes) %>% 
  inner_join( autores %>% group_by(pais) %>% summarise(f=n())
              , by=c("name"="pais") ) 

paises_corr_graph

paises_corr_graph %>% create_layout( layout = "kk") %>%
  ggraph::ggraph(layout) +
  geom_edge_link(aes(width = n), alpha = 0.2, color="gray") +
  geom_node_point(aes(size = f)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "coautoria entre paises (x frecuencia)",
       subtitle = "paises con mas de 50 articulos")


# en estos dos estoy perdiendo el weight, f o lo que sea

nodes <- paises_corr_graph %>% 
  activate(nodes) %>%
  inner_join(
    rownames_to_column(paises2, "name") %>% mutate(name=tolower(name))
    ) %>% 
  mutate(id=row_number()) %>%
  select(id,name,lon=x,lat=y) %>%
  as_tibble() 

edges <- paises_corr_graph %>% 
  activate(edges) %>% 
  #rename(weights=n) %>%
  distinct() %>%
  as_tibble() %>% 
  filter(from < 30, to < 30) #hay paises sin docs?

g <- graph_from_data_frame(d = edges, 
                           directed = FALSE, 
                           vertices = nodes)

edges_for_plot <- edges %>%
  inner_join(nodes %>% select(id, lon, lat), by = c('from' = 'id')) %>%
  rename(x = lon, y = lat) %>%
  inner_join(nodes %>% select(id, lon, lat), by = c('to' = 'id')) %>%
  rename(xend = lon, yend = lat)

nodes$weight = degree(g)

node_pos <- nodes %>%
  select(lon, lat) %>%
  rename(x = lon, y = lat)   # node positions must be called x, y

ggplot(nodes) + country_shapes +
  geom_curve(aes(x = x, y = y, xend = xend, yend = yend,
                 size=n),
             data = edges_for_plot, 
             curvature = 0.33, alpha = 0.5) +
  #scale_size_continuous(guide = FALSE) + # scale for edge widths
  #scale_size_continuous(guide = FALSE, range = c(0.25, 2)) + # scale for edge widths
  geom_point(aes(x = lon, y = lat, fill=weight),           # draw nodes
             shape = 21, 
             fill = 'white',
             color = 'black', stroke = 0.5) +
  #scale_size_continuous(guide = FALSE) +    # scale for node size
  #scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
  geom_text(aes(x = lon, y = lat, label = name, color=weight),             # draw text labels
            hjust = 0, nudge_x = 1, nudge_y = 4,
            #size = 3,
            #color = "white", 
            fontface = "bold") +
  mapcoords + maptheme

# va queriendo. tengo que poner el peso de los links, 
# y rever porque no está usa (debo estar usando correlaciones o algo asi)

```




In terms of authorship, most of the works agree that collaboratory authorship is the standard practice by far: only near 10-15% of the papers were written by a single author, more than 40% have at least 4 authors, and even there's a 1-2% of the papers that have more than 10 authors. Liu et. al. (2019) also suggest that single authorship ranks last in terms of impact. Most papers also agree that single authorship is declining, and that there seems to be a trend toward including more authors. Finally, XXXX suggests that collaboratory authorship is a requirement in big data research because of the costs and the difficulty of the data gathering and processing tasks involved.


### Coautoria e Índice de Subramanyam (IS) 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# cantidad de autores
autores %>% group_by(id) %>% 
  summarize(n=n()) %>% 
  group_by(n) %>% summarise(f=n()) %>% plot()

# mas de 10 autores
autores %>% group_by(id) %>% 
  summarize(n=n()) %>% filter(n>10) %>% arrange(desc(n)) %>% pull(id)

# promedio de autores por año
autores %>% left_join(
  metadata %>% filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(n=n()) %>%
  group_by(year) %>% summarise(avg=mean(n))

# cuantos autores por año
autores %>% left_join(
  metadata %>% 
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(autores=n()) %>%
  group_by(year,autores) %>% summarise(f=n()) %>%
  ggplot(aes(y=f,x=autores)) + geom_col() + facet_wrap(~year,scales = "free_y")

```

For the complete time frame (2008 − 2017), the average number of authors per
publication is 3.45 with a standard deviation of 2.34. 13.67% of articles are
published with a single authorship, and 41% of articles have more than the
average number of authors, i.e., 41% of publications have at least 4 authors.
The surprising aspect of the findings is the share of publications with more
than 10 authors. We found that 1% of the papers have more than 10 authors.
Fig. 4 summarizes the number of authors per publication distribution.

We also analyzed the evolution of number of authors per research publications over the selected time frame. We observe that during the initial years
(2008 − 2012) 27% of publications have single authorship. During the later
years, the single authorship trend declines. In 2016 only 11.6% of papers have
single authorship, the minimum for the selected time frame. Fig. 5 depicts the
authorship trends for the time frame. It should be noted that as the number of publications are significantly lower for initial years, the data for years
2008 − 2012 is combined for reporting purposes

---> aca sube la produccion y la cantidad de autores por articulo 
(ultimos años 1 no es el mas alto )

(se puede poner la media como linea vertical en cada año?)

INDICE DE XXXX

Corresponde a la proporción de artículos con autoría múltiple (2 o más autores).

Se calcula a partir de la siguiente ecuación:

IS = Nm / Nm + Ns

donde:
Nm = Número de artículos con autoría múltiple
Ns = Número de artículos con autoría simple (1 autor)
El valor máximo que puede alcanzar este índice es 1 y corresponde a que todos
los artículos publicados por la revista tienen al menos dos autores

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# cuantos autores por año
subra <- autores %>% left_join(
  metadata %>% 
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(autores=n()) %>%
  group_by(year,autores) %>% summarise(f=n()) %>%
  pivot_wider(names_from = year, values_from=f) %>% arrange(autores) 

print("Número de Artículos Distribuidos Según Número de Autores y por Período")
subra %>% janitor::adorn_totals(where = c("row","col")) 

# colSums(subra[,1:10], na.rm = TRUE) 
Ns <- colSums(subra[which(subra$autores==1),2:14], na.rm = TRUE)
Nm <- colSums(subra[which(subra$autores>1),2:14], na.rm = TRUE)
Nm / (Nm+Ns)

# estoy hay que integrarlo a la tabla anterior y agrupar los numeros de autores

```

Consistente con la idea de una tendencia hacia la colaboración, el índice de Subramanyam (IS), que calcula la proporción de artículos con dos o más autores sobre el total de documentos, muestra el progresivo incremento en la proporción de artículos en colaboración, desde un 54% en el período 1992-1996 hasta un 82% en el período 2012-2016. Por otra parte, el índice de Lawani (IL) permite complementar la noción acerca de la colaboración, calculando la media ponderada de autores por artículo en cada período. En este sentido, los resultados indican que junto con un incremento en la cantidad de artículos en coautoría, el incremento en la colaboración se ha traducido en un mayor número de autores por artículo, pasando de 1,9 autores en el período 1992-1996 a 3,4 en el período 2012-2016 (ver Tabla 3). Tomados en conjunto, ambos índices sugieren que no solo ha aumentado la proporción de artículos en los que aparecen dos o más autores firmantes, sino que también ha aumentado el número de autores firmantes por artículo.



## RQ2. topicos y areas

**RQ2**. Contents is the second issue to be explored. Most of the revised papers draw on keyword frequency and/or keyword correlation to identify topics and sub-areas of research. 

As expected, "big data" is the single most used keyword in most of the datasets. The exception that proves the rule is found at Kalantari et. al. (2017), which include papers from 1980s and whose search criteria include big data among others, but still show it as the most used keyword in the range 2010-2015. Belmonti et. al. (2020), on the other hand, highlight the centrality of machine learning.
Data-related tasks, such as mining, handling, storage, are the top keywords along with big data, according to Ahmad et. al. (2020). These are all general terms that originated in computer science but cannot be univocally confined to that space anymore. On the contrary, a topic that resides in computer science is the one that refer to tools and solutions for big data, such as Hadoop, MapReduce, or parallel and cloud computing. This is a topic that appear on most of the revised surveys. Authors agree that in the early days of big data the researchers focused mainly on its uses and applications, and on the technical challenges posed by it. 

Business intelligence, a topic closely related to big data applications and big data analytics, also appears across the surveys. Keywords that most resemble this topic are decision making, commerce, information management, and (business sector) management. According to Liang & Liu (2018), by 2014 management was one on the top keywords along with data mining, data science, which could be indicative of this topic closeness with the previous one; more specific terms, like big data analytics, knowledge management and others appear later. These authors also suggest that big data corpus is much more technical than business inteligence which is more application-oriented.

A third sub-area that appears on most surveys is related to analysis notions, such as machine learning, model, statistics, classification, or even artificial intelligence. Interestingly, most of these are topics that have been around for decades, whose keywords were the most frequent some years ago, and that lost prominence near 2015, but later resurged thanks to the re-discovery of their potential with big data. This trend has been registered by Kalantari et. al. (2019), Belmonte et. al. (2020), and Zhang et. al. (2019), who refer to some of these as "sleeping beauties". 

Ahmad et. al. (2020), Zhang et. al. (2019) and Liang & Liu (2018) identified a final topic that deals with big data integration with novel technologies that provide new sources and streams of data, such as Internet of Things (IoT), bioinformatics and social big data (e.g., social networks and apps). These are all emerging topics, absent in earlier surveys like the one by Kalantari et. al. (2017), although a few analytical techniques that originated years ago, such as sentiment analysis, opinion mining, or anomaly detection, have been revamped. We should remark that this topic seems to be the first one to include some "social" issues and concerns, such as privacy protection and human/patient care. 

Finally, Liang and Liu's (2018) paper include a brief report on social science and humanities corpus, constructed by subseting their big data and business intelligence corpus. The authors identified 10 topics by analyzing keywords and citation networks: the earliest incidence corresponds to medical-related issues, "where big data started in social sciences. It also echoes the fact that health care service is the most important field for big data applications"; the second topic, one that remains steady until 2015 (although it must be noted that their corpus is dated until 2017), renders big data as an emerging technology, referencing works like Kitchin (2014) that both presented the first comprehensive sociological analysis of big data and a critical programme for its analysis; another important topic is research on agenda setting, ilustrated with Lazer (2014), which spans between 2007 upto 2009. "In 2014, there were still many papers referring to agenda setting ... but almost none was cited after 2015. It may indicate that the studies involving agenda setting in big data had come to an end"; the rest of the topics they mention relate to business processes, and analysis techniques.


  * **RQ2.1** frecuencia de keywords
  * **RQ2.2** relaciones entre keywords (red y mapa semantico)
  * **RQ2.3** cuales son los temas? (topic modelling)
  * **XXXXX como se tematiza el big data? (para otro laburo!)**

aca usamos muchas mas tecnicas de analisis que los demas

### Keywords

```{r echo=FALSE, message=FALSE, warning=FALSE}


keywords <- metadata %>% filter(tag=="keyword")

keywords %>% group_by(value) %>% tally(sort = TRUE) %>% 
  #filter(value != "big data") %>%
  filter(n>100) %>% # keywords que se repitan en al menos 25 art
  # top_n(100) %>% view()
  ggplot(aes(y=n,x=reorder(value,n))) +
  geom_col() +
  coord_flip() 


library(wordcloud)
library(RColorBrewer)
library(wordcloud2)

print("wordcloud sin big data")

set.seed(1234) # for reproducibility 
# wordcloud(words = keywords %>% group_by(value) %>% tally(sort = TRUE) %>% pull(value),
#           freq = keywords %>% group_by(value) %>% tally(sort = TRUE) %>% pull(n), 
#           min.freq = 1,
#           max.words=100, random.order=FALSE, rot.per=0.35,
#           colors=brewer.pal(8, "Dark2"))

wordcloud2(data=keywords %>% count(value, sort = TRUE), size=1.6, color='random-dark')

```

Unlike other scientific fields that have clear taxonomy and classification of the sub- ject area [8], there is no classification scheme for big data. We use the frequency of keywords as metric to identify sub-areas and topics that gained the attention of researchers over the course of time. Our analysis found that the phrase “Big Data” is the most widely used keyword in our collection of records, which is unsurprising. The other keywords in order of occurrences are Data Mining (4995), Data Handling (4242) Digital Storage (3185), Cloud Computing (2921), Information Management (2795), Artificial Intelligence (2769), Distributed Computer Systems (2515), Learn- ing Systems (2261), and Algorithms (2027). In order to make the comparisons more realistic, we removed the keyword “Big Data” and designed a word cloud using the statistical software R (see Fig. 1)

In the early phase of big data research, the researcher focused mainly on uses and applications of big data research. For example, Jacob [39] discussed the challenges posed by the big data and highlighted possible solutions to overcome the challenges. Cohen et al. [16] discussed that the cost of data acquisition and storage has reduced considerably, and sophisticated data analysis has become a norm. They introduced Magnetic, Agile, Deep (MAD) data analysis practice. The proposed approach, design philosophy, and techniques were used to provide MAD analytics for Fox Audience Network. In addition, some works such as that of Brinkmann et al. [10] designed systems for large-scale data acquisition, processing, and storage. The authors claimed to collect 3 terabytes of data per day by performing continuous electrophysiological recordings of patients undergoing evaluation for epilepsy surgery. The huge amount of data generated posed storage, and processing challenges. Authors designed a platform that facilitated the acquisition, compression, and storage of large amount of data

After that, the researchers focused on application development for big data anal-
ysis as well as applications of big data in various domains. Major works in the area include Herodotou et al. [32], Chen et al. [14], Murdoch and Detsky [46], Hampton et al. [30], and Kramar et al. [1]. The diverse works focused on the developmental of new system for big data analysis [13,32], application of big data in health care [46], and ecological science [30], and studying emotional contagion in a large social networks [1]. In the later years, big data research focused on the integration of big data with
emerging technologies such as IoT [15], and social big data [7]. As the volume of the data is increasing exponentially, current research works are focusing on improving the execution times of data analysis techniques [21,45]. Similarly, the use ofGPU to solve complex big data analysis also needs further investigation [5]. Rodríguez-Mazahua et al. [51] identified data cleaning and data privacy as key issues in big data analysis.
4.4

### keywords relacionadas

```{r echo=FALSE, message=FALSE, warning=FALSE}

correlaciones <- keywords %>% 
  group_by(value) %>% filter(n() > 100) %>% ungroup() %>%  
  widyr::pairwise_cor(item = value, feature = id, sort = TRUE, method = "pearson") 

corr_graph <- tidygraph::as_tbl_graph(
  correlaciones %>% filter(correlation>=0.1), # correlaciones arriba de .1
  directed = FALSE) %>%
  activate(nodes) %>% 
  inner_join( keywords %>% group_by(value) %>% tally(), by=c("name"="value") ) 

corr_graph %>% create_layout( layout = "dh") %>%
  ggraph::ggraph(layout) + 
  geom_edge_link(aes(width = correlation), alpha = 0.2) + 
  geom_node_point(aes(size = n)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "Keyword correlation network") 

```

### mapa semantico de "big data"

esto del mapa semantico es un invento mio. ja.

```{r echo=FALSE, message=FALSE, warning=FALSE}

campo1 <- correlaciones %>% filter( item1 == "big data" ) %>% 
  top_n( n=25 , correlation ) %>% select( "item1", "item2" , "correlation" ) 
campo2 <- correlaciones %>% filter( item2 %in% campo1$item2 , item1 != "big data" ) %>% 
  top_n( n=100 , correlation ) %>% select( "item1", "item2" , "correlation" ) 
nodos1 <- campo1 %>% rename( item=item2 ) %>% distinct( item ) 
nodos2 <- campo2 %>% rename( item=item1 ) %>% distinct( item )
nodos2b <- dplyr::filter(nodos2, !item %in% intersect(nodos1$item, nodos2$item))      
if (nrow(nodos1) > 0){ nodos1$nivel=1 }
if (nrow(nodos2b) > 0){ nodos2b$nivel=2 } 
nodos<-rbind(nodos1,nodos2b)       
nodos<-rbind(nodos,c("big data" ,3))
semnet <- rbind(campo1, campo2) 
rm(nodos1,nodos2,nodos2b,campo1,campo2,nodos)

semgraph <- tidygraph::as_tbl_graph(semnet, directed = FALSE) %>%
  activate(nodes) %>% 
  left_join( keywords %>% group_by(value) %>% tally(), by=c("name"="value") ) 

semgraph %>% create_layout( layout = "gem") %>%
  ggraph::ggraph(layout) + 
  geom_edge_link(aes(width = correlation), alpha = 0.2) + 
  geom_node_point(aes(size = n)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "Keyword correlation to 'big data'",
    subtitle = "1 and 2 degree") 

semgraph %>% 
  activate(nodes) %>% 
  mutate(
    neighbors = centrality_degree(), # cantidad de vecinos
    centrality = centrality_authority() # centralidad entre vecinos
  ) %>%
  # filter(!node_is_isolated()) %>%
  mutate(
    # group = group_infomap(trials = 500),
    group = group_leading_eigen(steps = 100),
    # group = group_louvain() # ok
    # group = group_optimal() # ok
    # group = group_walktrap(steps = 10) 
  ) %>%
  create_layout( layout = "gem") %>%
  ggraph::ggraph(layout) + 
    geom_edge_link(aes(width = correlation), alpha = 0.1) + 
    geom_node_point(aes(size = n, color = factor(group))) +
    geom_node_text(aes(label = name), size = 4, repel = TRUE) +
    theme_graph() +
    labs(title = "Keyword correlation to 'big data'",
         subtitle = "1 and 2 degree") 

rm(semnet,semgraph)
rm(corr_graph, correlaciones, keywords)

```

### Topic modelling sobre abstracts

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(quanteda)
library(stm)
library(tidytext)
library(textstem)

limpiar <- function(txt) {
  txt <- tolower(txt)
  txt <- gsub("[^a-z ]","",txt)
  txt <- gsub("big data","big_data",txt, fixed = TRUE)
  txt <- gsub("â"," ",txt, fixed = TRUE)
  txt <- trimws(txt)
  return(txt)
}


abstracts <- metadata %>% filter(tag=="abstract") %>%
  mutate(value=limpiar(value))

tidy_abs <- abstracts %>%
  select(-tag,-source) %>%
  unnest_tokens(output = word, input = value) %>%
  anti_join(stop_words) %>% # remove stopwords
  mutate(len = str_length(word)) %>% filter(len>2) %>% select(-len) %>% # remove 1-2 char words
  mutate(word=textstem::lemmatize_words(x = word %>% unlist()))  # lemmatize


dfm_abs <- tidy_abs %>%
  count(id, word, sort = TRUE) %>%
  tidytext::cast_dfm(id, word, n)

dfm_abs_idf <- tidy_abs %>%
  count(id, word, sort = TRUE) %>%
  tidytext::bind_tf_idf(tbl = ., document = id, term = word, n = n)

# 2do: tengo que bajar tf_idf a tm
# https://www.tidytextmining.com/dtm.html

# tm basico con stm + tidy -----------------
# https://juliasilge.com/blog/sherlock-holmes-stm/

topic_model <- stm(dfm_abs, K = 20, verbose = FALSE, 
                   init.type = "Spectral", max.em.its = 2)

td_beta <- tidy(topic_model)

td_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
    term = reorder_within(term, beta, topic)) 

td_gamma <- tidy(topic_model, matrix = "gamma", 
  document_names = rownames(dfm_abs))

library(ggthemes)

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(100, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest(cols=c(terms))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
    mutate(topic = paste0("Topic ", topic),
      topic = reorder(topic, gamma))

gamma_terms %>%
  #top_n(50, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005 ) +
  coord_flip() +
  #scale_y_continuous(expand = c(0,0),
  #  limits = c(0, 0.09)) +
  theme_tufte(ticks = FALSE) +
  theme(plot.title = element_text(size = 16 ),
    plot.subtitle = element_text(size = 13)) +
  labs(x = NULL, y = expression(gamma),
    title = "Top topics by prevalence")

gamma_terms %>%
  select(topic, gamma, terms) %>%
  knitr::kable(digits = 3, 
    col.names = c("Topic", "Expected topic proportion", "Top 7 terms"))

# # cuales son los +terminos de cada topico?
# gamma_terms %>%
#   select(topic, gamma, terms)
# 
# # cuales son los +docs de cada topico?
# docs_topico <- td_gamma %>%
#   group_by(topic) %>%
#   arrange(desc(gamma)) %>% 
#   top_n(100)
# docs_topico

# 2do: falta mismo preprocesamiento a ambas tablas
# cuales / cuantas palabras del tm estan en los keywords
# cuantos keywords aparecen sobre el total de palabras x topic?
# y cuan exclusivos son? ----> esta puede ser la nocion conceptual a explorar?

rm(x)

```

presentar como tabla de temas con topkeywords

## RQ3. Citation



**RQ3**. The last issue we are interested in is citation, as a mean to identify influential works and venues.

In terms of citation distribution, Ahmad et. al. (2020) report that over 50% of publications are not cited, and that 80% of citations are received by aprox. 15% of the publications. The authors highlight that this statistics resemble those reported by Garounsi and Mäntylä (2016) for software engineering. Similarly, Kalantari et. al. (2017) used the Essential Science Indicators (ESI) tool provided by Thomson Reuters to identify 28 highly cited papers out of their dataset of 6,572, and reported average citations of 126.75 against 4.97.

Unsurprisingly, several of the papers reviewed report that the most cited works correspond to early survey papers, both on the general term of big data and on specific techniques, such as, "Big data: a survey" (Chen et. al., 2014), "Lexicon-based methods for sentiment analysis" (Taboada, et. al., 2011), or "Data mining with big data" (Wu et. al., 2014). These are all, arguably, technical papers that map the state of the art, and the upcoming challenges in the field.

A few interesting exceptions are some critical analysis, reflections and/or experimentations drawing on big social data, such as the study on "emotional contagion" in Facebook (Kramer, 2014) or the uses of Google for health trends detection (Lazer, et. al., 2014). In these, controversy may play an important factor, because of the claims received about the lack of informed consent and the impossibility to opt-out of the experiment by the former, and the introduction about "big data hubris" in the latter. Another exception is the excellent "Critical Questions for Big Data" by danah boyd and Kate Crawford (2014), which poses concerns about big data mythology and possible missuses. 

The predominance of computing and engineering papers is confirmed if we consult the top venues for publications, both in terms of citation and publication counts: Lecture Notes in Computer Science, Lecture notes in artificial intelligence, Proceedings of the VLDB Endowment, Bioinformatics, Procedia Computer Science, and the series published by IEEE. Top journals, with transdisciplinary orientation, such as Nature, Plos One, or the Proceedings of the National Academy of Sciences of the United States of America are also present. 

### top venues

We identify the top 10 venues for publications in terms of number of citations. In addition, we also record the number of publication made in each venue. Lecture Notes in Computer Science (LNCS)2 received the most number of citations (3064), which published the most number of articles as well (2538). This resulted in 1.20 citations per publication. LNCS is followed by Proceedings of the VLDB Endowment which received 2369 citations for 99 publications. The top 15 venues based on absolute cita- tion counts are given in Table 5. The table also lists the number of publications and citations/publication for each venue. In terms of citations per publication, MIS Quar- terly: Management Information Systems ranks at the top. However, a close inspection of the data reveals that the MIS Quarterly: Management Information Systems has received 1111 citations for 5 publications. Out of 1111 citations, 1098 citations are for [13], whichmeans that the rest of4 publications received only 13 citations. Informa- tion Communication and Society ranks 2 based on citations/publication; however, one publication [9] accrued 999 citations, whereas the remaining 13 publications acquired 86 citations. Other top venues based on the citation/publication include Proceedings of the National Academy of Sciences of the United States ofAmerica (66.9), Nature (43.3), and IEEE Transactions on Knowledge and Data Engineering (31.78)

Table 5 Top venues by most number of citations Venue
x Citation count 
x Publication count 
x Citations/publication


lo que trajimos de anystyle

```{r message=FALSE, warning=FALSE}

table(references$CATEGORY)

```

armamos nuestra propia tabla

```{r}

ref <- references %>% 
  filter(CATEGORY %in% c("ARTICLE","BOOK","INPROCEEDINGS")) %>% 
  select(AUTHOR, JOURNAL, TITLE, DATE, DOI, BOOKTITLE,id) %>%
  rowwise() %>% # bajar autores a string
      mutate(authors = paste(AUTHOR, collapse=', ')) %>%
      ungroup() %>%
  mutate(
    date_anystyle = as.integer(DATE), # fecha parseada por anystyle
    date_titulo = as.integer(sub("\\D*(\\d{4}).*", "\\1", TITLE)), # fecha del titulo
  ) %>%
  mutate(date = case_when(
    is.na(date_anystyle) & !is.na(date_titulo) ~ date_titulo,
    is.na(date_titulo) & !is.na(date_anystyle) ~ date_anystyle,
    !is.na(date_anystyle) & !is.na(date_titulo) ~ date_anystyle,
    is.na(date_anystyle) & is.na(date_titulo) ~ NA_integer_
  )) %>% 
  mutate( ref = paste(
      str_replace_all(string = authors, pattern = "[^[A-Za-z ,]]", replacement = ""),
      date,
      str_replace_all(string = TITLE, pattern = "[^[A-Za-z ]]", replacement = "") %>%
        str_sub(string = ., 1, 20)
      )
  ) %>%
  mutate( ref = trimws(tolower(ref)))

glimpse(ref)

# 2do: falta limpiar y borrar los na obvios

```
cantidad de referencias??? 

```{r}
ref %>% count(ref) %>% count(n, sort = TRUE) %>% plot()
```

### citas x año x articulos

What is the average number of citations/publications?

```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
  left_join(
    metadata %>% # citas por año
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
    mutate(year=as.integer(year)
  ) %>%
  count(year, name = 'references') %>%
  ungroup() %>%
  left_join(
    metadata %>% # articulos por año
    filter(tag=="year") %>%
    select(id,year=value) %>% 
    mutate(year=as.integer(year)) %>%
             count(year, name = 'articulos')
  ) %>%
  mutate(
    ref_art = references / articulos
  )

```


### Articulos más influyentes (+citas)


```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
  count(ref,sort = TRUE) %>% head()

```

#### publications in the selected time frame

**2do: calcular**

```{r echo=FALSE, message=FALSE, warning=FALSE}


```


#### Top cited publications for each year

```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
    left_join(
      metadata %>% 
      filter(tag=="year") %>%
      select(id,year=value)) %>% 
      mutate(year=as.integer(year)
    ) %>%
    group_by(year,ref) %>%
    summarise(n=n()) %>%
    filter(n>1) %>%
    slice_max(order_by = n, n = 5, with_ties = FALSE) %>%
  ggplot(aes(y=n,x=ref)) + geom_col() + facet_wrap(~year) + coord_flip()

```

#### Top 10 publications by normalized score

**2do: calcular**


```{r echo=FALSE, message=FALSE, warning=FALSE}

```


#### referencias por topicos (ya sea x tm o keywords)

**2do: calcular**


```{r echo=FALSE, message=FALSE, warning=FALSE}

```

# Conclusion

**meter comparacion con el general**





# Discussion

**significacion para mi proyecto**

(2) Contents: big data is a trans-disciplinary area of research with no clear taxonomy nor classifications. Analyzing content in a statistical manner can help us gain insights about the different ways in which it is framed and thematized in the different branches of knowledge;

Several papers have used bibliometric analysis to map the big data scientific production, with different goals and levels of analysis. With exception of a brief section from (), and to the best of our knowledge, this is the first study that focuses on social sciences, psychology and humanities, and provide a comparative view with the global production across disciplines.

With exception of a brief section from (XXXXXXXX), and to the best of our knowledge, this is the first study that focuses on social sciences, psychology and humanities, and provide a comparative view with the global production across disciplines.

**aportes del articulo**

Based on a sample size of 4,070 articles obtained from the Scopus database published between 2013 and 2018, this study revealed an array of interesting findings by utilizing the bibliometric approach and SciVal metrics. The results have provided a comprehensive understanding of big data research, in particular, in view of the scope, co-production and performance of this research stream.

This paper delivers a comprehensive review of research in BDA to foster a better understanding of a pervasive and new area. Several fundamental research questions that are relevant to current research efforts are focusing on the BDAA have been answered while providing a better platform for further evaluation and research in this area. A robust methodology called “Systematic Network Analysis of Literature” has been adopted for this purpose. This study conducted an SLR of 79 published papers and bibliometric analysis of 516 published papers from various journals, from 2014 to 2018. The research papers included in the review were selected based on the title, introduction, abstract, conclusion and research methods. A bibliometric analysis includes an integrated and systematic review of the literature to explore key themes and the theoretical framework underpinning journals, the field of inquiry or papers (Garc?ıa-Lillo et al., 2018). Due to their benefits over other approaches

## Limitations and future scope 

While this review paper has been completed rigorously, some limitations may present suggestions for future review papers. The data were collected from different journals, whereas the document examined did not include unpublished papers, master or doctoral theses and textbooks regarding BDAA. While this study has attempted to provide a comprehensive review based on the current literature, the data can be collected from the resources mentioned above as a recommendation for future studies, and the data reported and obtained in this study can be compared with the results obtained from those resources. This paper reviewed classified papers in the seven categories; it is suggested that future papers can classify and review papers in different areas and subcategories. Another limitation of this review was the extraction of all papers from English-language journals only. Consequently, the review did not include scientific journals in other languages. This study,
however, believes that most of the reviewed papers were from international journals. This study carefully selected and summarized the papers available in the Web of Science databases from several publishers. However, certain relevant outlets remained outside the scope of the current study. Therefore, future researchers may try to review papers not considered in the current review article. Another limitation is that in order to address the second research question, BDAA in different countries, the bibliometric analysis was carried out with only specific keywords to select Webof Science database papers; future researchmay aim at using different keywords and objectives to obtain different results. In the future effect of BDA on supply chain risk and flexibility can be studied. Role of artificial intelligence (AI) on BDA can be further analyze
It is important to present the limitation of the study, as it might be possible that the results are not obtained if the same set of experiments is repeated again. We downloaded the data from Scopus on January 27, 2018. As Scopus has data download limits, the data were downloaded in an incremental manner and later combined. It is possible that the reader may find the number of citations for various articles different than the ones reported here. There can be several reasons for this. For example, the article might have accumulated more citations over time. It is also possible that various sources might report different statistics for the same article. A noticeable example is the case of publication“Data mining with big data.” The paper according to Scopus has received 681 citations, whereas Google Scholar has recorded 1320 citations for the publication till 2017. The publisher IEEE Transactions on Knowledge and Data Engineering recorded 524 citations for the same publication. The difference between the citation count of Scopus and other sources can be attributed to the fact that Scopus citations are based on Scopus-indexed publications only. It is also important to mention that Google Scholar citations also include non-academic citations. As stated earlier that a variety of past studies has validated the authenticity of Scopus, therefore, our results are also based on the statistics of Scopus only

# Referencias

