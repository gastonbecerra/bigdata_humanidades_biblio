---
title: "Bibliometrico socio y humanidades sobre big data"
author: "gaston becerra"
date: "20/5/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  comment = '', fig.width = 8, fig.height = 3
  )

options(scipen = 999)

library(tidyverse)
library(igraph)
library(ggraph)
library(tidygraph)

metadata <- readr::read_csv(file = "../data/metadata.csv")
ocr <- readr::read_csv(file = "../data/ocr.csv")
autores <- readr::read_csv(file = "../data/autores.csv")
references <- readRDS(file = "../data/referencias/referencias.rds")

```


# Abstract


***Purpose**: In recent years, the rapid growth of big data has presented immense potential for business applications as well as raised great interest from academia. In response to this emerging phenomenon, the purpose of this paper is to provide a comprehensive literature review of big data.*
***Design/methodology/approach**: A bibliometric method was used to analyze the articles obtained from the Scopus database published between 2013 and 2018. A sample size of 4,070 articles was evaluated using SciVal metrics.* 
***Findings**: The analysis revealed an array of interesting findings as follows: 
- the number of publications related to big data increased steadily over the past six years, though the rate of increase has slowed since 2014; 
- the scope of big data research is quite broad in regards to both research domains and countries; 
- despite a large volume of publications, the overall performance of big data research is not well presented as measured by the field-weighted citation impact metric; - collaboration between different institutions, particularly in the form of international collaboration and academic–corporate collaboration, has played an important role in improving the performance of big data research.
- over 50% of publications do not receive any citations, and the average number of citations per publication is 3.17. 
- It is also observed that single authorship of research publications has declined over the time. 
- The analysis reveals the pioneering role played by the USA in advancing the research in big data, which has lately been taken over by China, and the large-scale usage of big data analytics in various domains of science.*
***Originality/value**: To the best of the authors’ knowledge, this is the first study to provide a holistic view of the big data research. The insights obtained from the analysis are instrumental for both academics and practitioners.*

**Keywords**: Big data, social sciences, humanities, bibliometric analysis, citation analysis


# Introduction

The objective of this work is to investigate the evolution of big data literature
using bibliometric analysis. Bibliometrics is a quantitative tool for the analysis of literature published in a scientific field. Bibliometrics and citation analysis is used in various fields to identify the most influentialwork and researchers, and in the analysis and evolution ofa specific research theme [17,55].

The aim is to identify the most influential work, top venues for publications, citation trends, geographical and institutional trends, as well as authorship trends in literature published in the domain of big data. The study is based on the articles published in the period of 10 years (2008–2017) and covers over 35,000 records. The rest ofthe article is organized as follows: Section 2 presents a brief review of studies in the field of bibliometrics. Section 3 presents research questions, and the methodology for data extraction. Section 4 presents an in-depth analysis of the data. Section 5 discusses the limitation of the study, and finally, Sect. 6 concludes the work

Using Scopus as the data source, we perform a thorough analysis of scholarly works published in the field of big data from 2008 to 2017. 

# Big data literature review 

(obs de segundo orden sobre tematizaciones)

Big data research has seen a greater interest in the last decade, and attracted researchers from transdisciplinary areas such as physical sciences, natural sciences, social sciences, and biomedical sciences [2,4,13,28,38,42,49,50]. The concept of big data originated from the information explosion that occurred because of widespread adaptation of information and communication technologies.

The term big data appeared in the late 1990s when it was observed that many data sets were too large to be processed by standard algorithms and software (Cox and Ellsworth, 1997). Currently, there are different definitions of big data, such as “large growing data sets that include heterogeneous formats: structured, unstructured and semi-structured data” (Oussous et al., 2018, p. 433); “a large and complex collection of data sets, which is difficult to process using on-hand database management tools and traditional data processing applications” (Furht and Villanustre, 2016, p. 3). However, a common idea of big data, and what distinguishes it from traditional data information, has been already established. Big data has been endowed with many characteristics, such as volume, velocity, variety,
value and veracity (Gandomi and Haider, 2015; Laney, 2001; Manogaran et al., 2016). Normally, volume is understood as the size of the data; velocity is the rate at which data are generated and the speed; variety refers to the various data formats; value is the benefits obtained from the data, and veracity is the quality of big data. These characteristics provide a better understanding of big data and allow for the big data to be exploited more effectively

Sintetizar Ahmad, I., Ahmed, G., Shah, S. A. A., & Ahmed, E. (2020). A decade of big data literature: analysis of trends in light of bibliometrics. Journal of Supercomputing, 76(5), 3555–3571. https://doi.org/10.1007/s11227-018-2714-x

There is a plethora of work dedicated to citation analysis in various fields [17,19,24, 27,34]. We briefly discuss works in bibliometrics in relation to “big data” and then present major scholarly publications in citation analysis.

Nobre and Tavares [48] analyzed the literature related to the application of big
data/IoT in the context of circular economy indexed in Scopus for the time frame 2006–2015. The study found that China and USA are the most active countries. Sur- prisingly, among countries producing large greenhouse gases, Brazil and Russia were not contributing much in terms of number of publications in big data. Kalantari [40] performed bibliometric analysis of 6572 papers indexed inWeb of Science from 1980 to March 19, 2015. Using MS Excel, general concentration, dispersion, and move- ment of the data from the selected pool were analyzed. Liao et al. [43] performed bibliometric analysis of big data literature published in the field of medical big data. The authors used Science Citation Index Expanded and the Social Science Citation Index databases as data sources to extract 988 references. Therewere no restrictions on the time span. The novelty of the work is the application of multi-regression analysis considering the number of authors, number of pages, and number of references. It was observed that the medical big data literature has seen a rise after 2010. By analysis of the keywords, it was identified that the medical care is shifting its focus toward patient-centered model than disease-centered approach.

Hoonlor et al. [35] performed an in-depth analysis of citation data in computer sci-
ence. They inferred that most publications mention the keyword “algorithm,” and most abstracts are related to databases, neural networks, and Internet. The study also identi- fiedweb as an attractive source ofdata and application test beds,which resulted inmore research in the areas of data mining, cloud computing, and information retrieval. The study also concluded that funding is essentially required to keep research momentum and progress in a specific field

Effendy and Yap [22] performed the trend analysis of research areas in computer
science using Microsoft Academic Graph dataset. The authors proposed a new metric called FoS score to measure the level of interest in a specific research topic. Using the measure, they discussed citation trends, trends in conferences, evolution of research areas, and the relation between research areas.


# Methodology

## Research questions

Following the pattern of Garousi and Fernandes [26], we formulate a set of research questions and base our analysis of the data on these key questions. The main objective of choosing the research questions is to identify the top cited papers, the contributions of various countries in advancing big data research, and identification of key research areas. The set of research questions are as following

**RQ1** Cuál es la dinámica  de autoría y colaboración? 

**RQ1.1** What is the average number of authors per publications per year? 
**RQ1.2** Cuales son los paises que más publicaron? Cómo es la colaboración entre paises?
**RQ1.3** Cuales son las instituciones con mas articulos en academia / fuera? Como es la colaboracion entre instituciones?

**RQ2** What are the key topics/areas that are addressed in publications? 

**RQ2.1** frecuencia de keywords
**RQ2.2** relaciones entre keywords (red y mapa semantico)
**RQ2.3** cuales son los temas? (topic modelling)

**RQ3** Citaciones

**RQ3.1** top venues <------------- no esta hecho
**RQ3.2** What are the top cited publications for each year in the selected time frame? Articulos más influyentes (+citas)
**RQ3.3** cuantas citas por año?

(preguntar citas a clasicos?)

## Procedure and Data Analysis

The research process was carried out in different actions. First the database was selected. In this
case, WoS was chosen as a database that contains a large number of indexed impact studies. Next, the keywords to be analyzed were determined. In this study the terms “Machine Learning” and “Big Data” were chosen, after consultation in various specialized thesauri. Next, the search equation was constructed. The result was “Machine Learning” [TOPIC] AND “Big Data” [TOPIC] with the intention of refining the process of reporting scientific documents that had such terms in title, abstract, and keywords of indexed publications. These first actions obtained a scientific production of 4328 documents. The first studies dated
back to the year 2010. Therefore, the literature of the last 10 years (2010-2019) was taken, suppressing studies published in 2020 (n = 74) for not having finished the year and duplicates or indexed incorrectly (n = 14). Therefore, the unit of analysis focused on 4240 documents. This figure was the result of the application of various production indicators with their respective inclusion criteria such as year of publication (all production except 2020), language (x ≥ 10), publication area (x ≥ 700), type of documents (x ≥ 100), organizations (x ≥ 50), authors (x ≥ 10), sources of origin (x ≥ 30), countries (x ≥ 200), citation (the four most cited documents; x ≥ 250). 
To analyze the reported literature, various software was used. Two are tools fromWoS, Analyze
Results and Creation Citation Report. These were used to extract the data related to the year, authorship, country, type of document, institution, language, medium and most cited documents. The other program was SciMAT, used to longitudinally analyze the structural and dynamic development of scientific production. For an effective analysis, the instructions of experts in this latest software were followed [34]. SciMAT allowed the following thematic co-word analysis to be carried out through the following processes
A key consideration in any citations-based study is the selection of data source. A number of online databases are available that provide access to the citation data. These sources include ISI Web of Knowledge, Scopus, Google Scholar and dblp. Out of these, ISI Web of Knowledge and Scopus are the two main sources used by the majority of researchers for citation data analysis [17,24,25,33,34]. Other citation databases are also used such as dblp by Hoonlor et al. [35]. Our choice for data collection is Scopus because of service availability, ease of use and authenticity of the data [26].
After selection of the source, the next step is the extraction of the required data from
the data source. Scopus provides a flexible and customized way for data extraction from its database using various criteria such as search by author name, source name, affiliation, and keywords search. We used title and keywords as our main search criterion, i.e., we queried the database to return all documents where the word “Big Data” is located either in the title of the document or in the associated keywords. We also restricted the results to English language articles and articles published between 2008 and 2017 (both inclusive). The query is given in Table 1
Scopus has indexed 13 documents prior to 2008 satisfying our search criterion.
Out of these, only a single document1 has received 19 citations. Likewise, no more than 3 papers/year are indexed by Scopus prior to 2008. Therefore, we chose 2008 as starting year for our data extraction. Thus, the time frame spans a decade of research in the field. Our search criteria have resulted in total of 34,655 documents. Using the graphical user interface provided by Scopus, we exclude some document types namely conference review, editorial notes, and letters. The main motivation behind the omission of these document type resides in the fact that they mostly provide information about the outlet such as scope of the conference/special issue, the number of submissions, and the acceptance rate, and normally have very low citation counts. The omission of these documents reduced the documents number to 33,623. As stated earlier that big data is a transdisciplinary field, we did not limit to literature from computer science only and allowed results fromdiverse fields such as business, medical science, and social sciences. Note that the data are downloaded on January 27, 2018, and the citation count might slightly differ at later dates. The dataset is stored in comma-separated values (CSV) format, and analysis is performed in the R statistical tool.

## Dataset

The timeframe for retrieving the academic publications from the Scopus database was set from the most recent six years, i.e. 2013–2018. A set of standards were created in order to obtain a suitable sample, i.e. language is English; title and keywords contain “big data”; and the document is labeled as “articles.” A total of 4,150 articles were obtained by following this process. Among these articles,
80 articles which presented incomplete information were deleted. A sample size of 4,070 was finally used for analysis. Figure 1 exhibits the procedure for data collection

The total number of articles found was XXXX
As it can be noted in Graph 1, the greatest production took place in the xxxxx

```{r tiempo, echo=FALSE, message=FALSE, warning=FALSE}

table(metadata %>% filter(tag=="year") %>% pull(value)) %>% plot()

```


# Results

## RQ1. Autores / Colaboracion

### Coautoria e Índice de Subramanyam (IS) 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# cantidad de autores
autores %>% group_by(id) %>% 
  summarize(n=n()) %>% 
  group_by(n) %>% summarise(f=n()) %>% plot()

# mas de 10 autores
autores %>% group_by(id) %>% 
  summarize(n=n()) %>% filter(n>10) %>% arrange(desc(n)) %>% pull(id)

# promedio de autores por año
autores %>% left_join(
  metadata %>% filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(n=n()) %>%
  group_by(year) %>% summarise(avg=mean(n))

# cuantos autores por año
autores %>% left_join(
  metadata %>% 
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(autores=n()) %>%
  group_by(year,autores) %>% summarise(f=n()) %>%
  ggplot(aes(y=f,x=autores)) + geom_col() + facet_wrap(~year,scales = "free_y")

```

For the complete time frame (2008 − 2017), the average number of authors per
publication is 3.45 with a standard deviation of 2.34. 13.67% of articles are
published with a single authorship, and 41% of articles have more than the
average number of authors, i.e., 41% of publications have at least 4 authors.
The surprising aspect of the findings is the share of publications with more
than 10 authors. We found that 1% of the papers have more than 10 authors.
Fig. 4 summarizes the number of authors per publication distribution.

We also analyzed the evolution of number of authors per research publications over the selected time frame. We observe that during the initial years
(2008 − 2012) 27% of publications have single authorship. During the later
years, the single authorship trend declines. In 2016 only 11.6% of papers have
single authorship, the minimum for the selected time frame. Fig. 5 depicts the
authorship trends for the time frame. It should be noted that as the number of publications are significantly lower for initial years, the data for years
2008 − 2012 is combined for reporting purposes

---> aca sube la produccion y la cantidad de autores por articulo 
(ultimos años 1 no es el mas alto )

(se puede poner la media como linea vertical en cada año?)

INDICE DE XXXX

Corresponde a la proporción de artículos con autoría múltiple (2 o más autores).

Se calcula a partir de la siguiente ecuación:

IS = Nm / Nm + Ns

donde:
Nm = Número de artículos con autoría múltiple
Ns = Número de artículos con autoría simple (1 autor)
El valor máximo que puede alcanzar este índice es 1 y corresponde a que todos
los artículos publicados por la revista tienen al menos dos autores

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# cuantos autores por año
subra <- autores %>% left_join(
  metadata %>% 
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
  group_by(year,id) %>% summarise(autores=n()) %>%
  group_by(year,autores) %>% summarise(f=n()) %>%
  pivot_wider(names_from = year, values_from=f) %>% arrange(autores) 

print("Número de Artículos Distribuidos Según Número de Autores y por Período")
subra %>% janitor::adorn_totals(where = c("row","col")) 

# colSums(subra[,1:10], na.rm = TRUE) 
Ns <- colSums(subra[which(subra$autores==1),2:14], na.rm = TRUE)
Nm <- colSums(subra[which(subra$autores>1),2:14], na.rm = TRUE)
Nm / (Nm+Ns)

# estoy hay que integrarlo a la tabla anterior y agrupar los numeros de autores

```

Consistente con la idea de una tendencia hacia la colaboración, el índice de Subramanyam (IS), que calcula la proporción de artículos con dos o más autores sobre el total de documentos, muestra el progresivo incremento en la proporción de artículos en colaboración, desde un 54% en el período 1992-1996 hasta un 82% en el período 2012-2016. Por otra parte, el índice de Lawani (IL) permite complementar la noción acerca de la colaboración, calculando la media ponderada de autores por artículo en cada período. En este sentido, los resultados indican que junto con un incremento en la cantidad de artículos en coautoría, el incremento en la colaboración se ha traducido en un mayor número de autores por artículo, pasando de 1,9 autores en el período 1992-1996 a 3,4 en el período 2012-2016 (ver Tabla 3). Tomados en conjunto, ambos índices sugieren que no solo ha aumentado la proporción de artículos en los que aparecen dos o más autores firmantes, sino que también ha aumentado el número de autores firmantes por artículo.

### Colaboración entre paises

antes que nada: Which countries and institutions have contributedmost in terms ofpublications count?

```{r echo=FALSE, message=FALSE, warning=FALSE}

autores %>% 
  filter(!is.na(pais)) %>%
  count(pais, sort = TRUE) %>% 
  head(30) %>%
  ggplot(aes(y=n,x=reorder(pais,n))) + geom_col() +
  coord_flip() +
  labs(title="ranking +30 paises")

# autores %>% 
#   left_join(
#   metadata %>% 
#     filter(tag=="year") %>%
#     select(id,year=value)) %>%
#   filter(!is.na(pais)) %>%
#   count(pais, year, sort = TRUE) %>% 
#   pivot_wider(names_from = year, values_from=n) %>%
#   head(10)

autores %>% 
  left_join(
  metadata %>% 
    filter(tag=="year") %>%
    select(id,year=value)) %>%
  filter(!is.na(pais)) %>% 
  count(pais, year, sort = TRUE) %>% 
    filter(n>1) %>%
    ggplot(aes(x=year,y=pais,fill=n)) + geom_tile()

```


```{r colaboracion-mapa}


# library(ggmap)
# Sys.setenv(google_key = "AIzaSyAGSGP7CUWpxDJkxjVe_UKGaV0U1Ea0gn0")
# register_google(key = Sys.getenv("google_key"))
# pp <- V(paises_corr_graph) %>% as.character()
# geoloc <- ggmap::geocode(location = pp)
# geoloc$pais <- pp
# geoloc %>% write.csv("../data/geoloc.csv")

# https://datascience.blog.wzb.eu/2018/05/31/three-ways-of-visualizing-a-graph-on-a-map/
# https://rstudio-pubs-static.s3.amazonaws.com/339257_fd4db2366a1f4e44ae95a0fd6db24457.html
# http://dyerlab.github.io/popgraph/

paises <- autores %>% 
  filter(!is.na(pais)) %>%
  count(pais, sort = TRUE)

glimpse(paises)

library(rgeos)
library(rworldmap)

wmap <- getMap(resolution="high")
centroids <- gCentroid(wmap, byid=TRUE)
paises2 <- as.data.frame(centroids)
map_world <- map_data(map = "world") %>%
  filter(region != "Antarctica")

maptheme <- theme(panel.grid = element_blank()) +
  theme(axis.text = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(axis.title = element_blank()) +
  theme(legend.position = "bottom") +
  theme(panel.grid = element_blank()) +
  theme(panel.background = element_rect(fill = "#596673")) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'cm'))

country_shapes <- geom_polygon(aes(x = long, y = lat, group = group),
                               data = map_data('world'),
                               fill = "#c9c9c9", color = "#515151",
                               size = 0.15)
mapcoords <- coord_fixed(xlim = c(-150, 180), ylim = c(-55, 80))





paises_corr <- autores %>% 
  filter(!is.na(pais)) %>%
  group_by(pais) %>% filter(n() > 50) %>% ungroup() %>%  
  widyr::pairwise_count(item = pais, feature = id, sort = TRUE) 

paises_corr_graph <- tidygraph::as_tbl_graph(
  paises_corr,
  directed = FALSE) %>%
  activate(nodes) %>% 
  inner_join( autores %>% group_by(pais) %>% summarise(f=n())
              , by=c("name"="pais") ) 

paises_corr_graph

paises_corr_graph %>% create_layout( layout = "kk") %>%
  ggraph::ggraph(layout) +
  geom_edge_link(aes(width = n), alpha = 0.2, color="gray") +
  geom_node_point(aes(size = f)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "coautoria entre paises (x frecuencia)",
       subtitle = "paises con mas de 50 articulos")


# en estos dos estoy perdiendo el weight, f o lo que sea

nodes <- paises_corr_graph %>% 
  activate(nodes) %>%
  inner_join(
    rownames_to_column(paises2, "name") %>% mutate(name=tolower(name))
    ) %>% 
  mutate(id=row_number()) %>%
  select(id,name,lon=x,lat=y) %>%
  as_tibble() 

edges <- paises_corr_graph %>% 
  activate(edges) %>% 
  #rename(weights=n) %>%
  distinct() %>%
  as_tibble() %>% 
  filter(from < 30, to < 30) #hay paises sin docs?

g <- graph_from_data_frame(d = edges, 
                           directed = FALSE, 
                           vertices = nodes)

edges_for_plot <- edges %>%
  inner_join(nodes %>% select(id, lon, lat), by = c('from' = 'id')) %>%
  rename(x = lon, y = lat) %>%
  inner_join(nodes %>% select(id, lon, lat), by = c('to' = 'id')) %>%
  rename(xend = lon, yend = lat)

nodes$weight = degree(g)

node_pos <- nodes %>%
  select(lon, lat) %>%
  rename(x = lon, y = lat)   # node positions must be called x, y

ggplot(nodes) + country_shapes +
  geom_curve(aes(x = x, y = y, xend = xend, yend = yend,
                 size=n),
             data = edges_for_plot, 
             curvature = 0.33, alpha = 0.5) +
  #scale_size_continuous(guide = FALSE) + # scale for edge widths
  #scale_size_continuous(guide = FALSE, range = c(0.25, 2)) + # scale for edge widths
  geom_point(aes(x = lon, y = lat, fill=weight),           # draw nodes
             shape = 21, 
             fill = 'white',
             color = 'black', stroke = 0.5) +
  #scale_size_continuous(guide = FALSE) +    # scale for node size
  #scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
  geom_text(aes(x = lon, y = lat, label = name, color=weight),             # draw text labels
            hjust = 0, nudge_x = 1, nudge_y = 4,
            #size = 3,
            #color = "white", 
            fontface = "bold") +
  mapcoords + maptheme

# va queriendo. tengo que poner el peso de los links, 
# y rever porque no está usa (debo estar usando correlaciones o algo asi)

```



### ranking afiliaciones

```{r echo=FALSE, message=FALSE, warning=FALSE}

academicAff <- "universi|college|school|institu|academ|politec"

cleanAff <- function(txt) {
  z <- c()
  for(i in 1:length(txt)){
    if (!is.na(txt)){
      txt2 <- str_split(string = txt[i], pattern = ",", simplify = TRUE) %>%
        as.character()
      txt3 <- grepl(
          pattern = academicAff, 
          txt2, ignore.case = TRUE)
      if(TRUE %in% txt3) {
        z <- c(z, trimws(txt2[max(which(txt3==TRUE))]))  
      } else {
        z <- c(z, trimws(txt2[1]))
      }
    } else {
      z <- c(z, NA)
    }
  } 
  return(z)  
}

autores2 <- autores %>% 
  filter(!is.na(aff)) %>%
  mutate(
    uni = grepl(
      pattern = academicAff, 
      aff, ignore.case = TRUE),
    aff2 = cleanAff(aff)
    ) 

autores2 %>% count(uni,sort=TRUE) 

autores2 %>% 
  count(uni,aff2) %>% group_by(uni) %>% 
  mutate(aff2=str_sub(aff2,1,15))%>%
  slice_max(n = 20, order_by=n) %>%
  ggplot(aes(y=n,x=reorder(aff2,n),fill=uni)) + geom_col() +
  coord_flip() +
  facet_wrap(~uni,scales = "free")+
  labs(title="ranking universidades y empresas")

autores2 %>% 
  filter(!is.na(aff)) %>%
  count(aff2, uni, sort = TRUE) %>% 
  mutate(aff2=str_sub(aff2,1,15))%>%
  ggplot(aes(y=n,x=reorder(aff2,n),fill=uni)) + geom_col() +
  facet_wrap(~uni,scales = "free")+
  labs(title="ranking +30 aff que son o no universidades")+
  coord_flip()

# en todos estos calculos hay que ver si estamos hablando de instituciones
# o de investigadores en instituciones

```

### Colaboración académica-corporativa

Co-authorship by ACC. Among the collaborative papers, 112 (2.8 percent) are
produced through the manner of ACC. Due to the small amount of publications on this topic, this finding is likely to indicate that, in the field of big data research, joint work contributed by both academia and industry is not popular. 
Table I outlines the top 10 institutions and countries/regions in the form of ACC publication. As listed in Table I, six of the institutions are affiliated with the USA, whereas the other four are relevant to China and Switzerland. In terms of countries, the USA ranks at the top with over half ACC publications (60/54 percent). China (24/21 percent), the UK (14/13 percent) and Canada (12/11 percent) have published more than ten ACC articles. The countries do not have a big representation in ACC articles.


```{r echo=FALSE, message=FALSE, warning=FALSE}

acc <- autores2 %>% count(id,uni) %>% 
  pivot_wider(names_from = uni, values_from = n) %>%
  rename(academy=2,private=3) %>%
  mutate( acc=case_when(
    !is.na(academy) & !is.na(private) ~ "acc",
    !is.na(academy) & is.na(private) ~ "pure_academy",
    is.na(academy) & !is.na(private) ~ "pure_private"
    )
  )

acc %>% count(acc)  # cuantos puros y cuantos acc

autores2 %>% inner_join(
    acc %>% select(id,acc)
  ) %>%
  filter(acc=="acc") %>%
  count(uni,aff2) %>% group_by(uni) %>% 
  slice_max(n = 20, order_by=n) %>%
  mutate(aff2=str_sub(aff2,1,15))%>%
  ggplot(aes(y=n,x=reorder(aff2,n),fill=uni)) + geom_col() +
  coord_flip() +
  facet_wrap(~uni,scales = "free_y")+
  labs(title="top instituciones que hacen ACC")

```


## RQ2. topicos y areas


### Keywords

```{r echo=FALSE, message=FALSE, warning=FALSE}


keywords <- metadata %>% filter(tag=="keyword")

keywords %>% group_by(value) %>% tally(sort = TRUE) %>% 
  #filter(value != "big data") %>%
  filter(n>100) %>% # keywords que se repitan en al menos 25 art
  # top_n(100) %>% view()
  ggplot(aes(y=n,x=reorder(value,n))) +
  geom_col() +
  coord_flip() 


library(wordcloud)
library(RColorBrewer)
library(wordcloud2)

print("wordcloud sin big data")

set.seed(1234) # for reproducibility 
# wordcloud(words = keywords %>% group_by(value) %>% tally(sort = TRUE) %>% pull(value),
#           freq = keywords %>% group_by(value) %>% tally(sort = TRUE) %>% pull(n), 
#           min.freq = 1,
#           max.words=100, random.order=FALSE, rot.per=0.35,
#           colors=brewer.pal(8, "Dark2"))

wordcloud2(data=keywords %>% count(value, sort = TRUE), size=1.6, color='random-dark')

```

Unlike other scientific fields that have clear taxonomy and classification of the sub- ject area [8], there is no classification scheme for big data. We use the frequency of keywords as metric to identify sub-areas and topics that gained the attention of researchers over the course of time. Our analysis found that the phrase “Big Data” is the most widely used keyword in our collection of records, which is unsurprising. The other keywords in order of occurrences are Data Mining (4995), Data Handling (4242) Digital Storage (3185), Cloud Computing (2921), Information Management (2795), Artificial Intelligence (2769), Distributed Computer Systems (2515), Learn- ing Systems (2261), and Algorithms (2027). In order to make the comparisons more realistic, we removed the keyword “Big Data” and designed a word cloud using the statistical software R (see Fig. 1)

In the early phase of big data research, the researcher focused mainly on uses and applications of big data research. For example, Jacob [39] discussed the challenges posed by the big data and highlighted possible solutions to overcome the challenges. Cohen et al. [16] discussed that the cost of data acquisition and storage has reduced considerably, and sophisticated data analysis has become a norm. They introduced Magnetic, Agile, Deep (MAD) data analysis practice. The proposed approach, design philosophy, and techniques were used to provide MAD analytics for Fox Audience Network. In addition, some works such as that of Brinkmann et al. [10] designed systems for large-scale data acquisition, processing, and storage. The authors claimed to collect 3 terabytes of data per day by performing continuous electrophysiological recordings of patients undergoing evaluation for epilepsy surgery. The huge amount of data generated posed storage, and processing challenges. Authors designed a platform that facilitated the acquisition, compression, and storage of large amount of data

After that, the researchers focused on application development for big data anal-
ysis as well as applications of big data in various domains. Major works in the area include Herodotou et al. [32], Chen et al. [14], Murdoch and Detsky [46], Hampton et al. [30], and Kramar et al. [1]. The diverse works focused on the developmental of new system for big data analysis [13,32], application of big data in health care [46], and ecological science [30], and studying emotional contagion in a large social networks [1]. In the later years, big data research focused on the integration of big data with
emerging technologies such as IoT [15], and social big data [7]. As the volume of the data is increasing exponentially, current research works are focusing on improving the execution times of data analysis techniques [21,45]. Similarly, the use ofGPU to solve complex big data analysis also needs further investigation [5]. Rodríguez-Mazahua et al. [51] identified data cleaning and data privacy as key issues in big data analysis.
4.4

### keywords relacionadas

```{r echo=FALSE, message=FALSE, warning=FALSE}

correlaciones <- keywords %>% 
  group_by(value) %>% filter(n() > 100) %>% ungroup() %>%  
  widyr::pairwise_cor(item = value, feature = id, sort = TRUE, method = "pearson") 

corr_graph <- tidygraph::as_tbl_graph(
  correlaciones %>% filter(correlation>=0.1), # correlaciones arriba de .1
  directed = FALSE) %>%
  activate(nodes) %>% 
  inner_join( keywords %>% group_by(value) %>% tally(), by=c("name"="value") ) 

corr_graph %>% create_layout( layout = "dh") %>%
  ggraph::ggraph(layout) + 
  geom_edge_link(aes(width = correlation), alpha = 0.2) + 
  geom_node_point(aes(size = n)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "Keyword correlation network") 

```

### mapa semantico de "big data"

esto del mapa semantico es un invento mio. ja.

```{r echo=FALSE, message=FALSE, warning=FALSE}

campo1 <- correlaciones %>% filter( item1 == "big data" ) %>% 
  top_n( n=25 , correlation ) %>% select( "item1", "item2" , "correlation" ) 
campo2 <- correlaciones %>% filter( item2 %in% campo1$item2 , item1 != "big data" ) %>% 
  top_n( n=100 , correlation ) %>% select( "item1", "item2" , "correlation" ) 
nodos1 <- campo1 %>% rename( item=item2 ) %>% distinct( item ) 
nodos2 <- campo2 %>% rename( item=item1 ) %>% distinct( item )
nodos2b <- dplyr::filter(nodos2, !item %in% intersect(nodos1$item, nodos2$item))      
if (nrow(nodos1) > 0){ nodos1$nivel=1 }
if (nrow(nodos2b) > 0){ nodos2b$nivel=2 } 
nodos<-rbind(nodos1,nodos2b)       
nodos<-rbind(nodos,c("big data" ,3))
semnet <- rbind(campo1, campo2) 
rm(nodos1,nodos2,nodos2b,campo1,campo2,nodos)

semgraph <- tidygraph::as_tbl_graph(semnet, directed = FALSE) %>%
  activate(nodes) %>% 
  left_join( keywords %>% group_by(value) %>% tally(), by=c("name"="value") ) 

semgraph %>% create_layout( layout = "gem") %>%
  ggraph::ggraph(layout) + 
  geom_edge_link(aes(width = correlation), alpha = 0.2) + 
  geom_node_point(aes(size = n)) +
  geom_node_text(aes(label = name), size = 4, repel = TRUE) +
  theme_graph() +
  labs(title = "Keyword correlation to 'big data'",
    subtitle = "1 and 2 degree") 

semgraph %>% 
  activate(nodes) %>% 
  mutate(
    neighbors = centrality_degree(), # cantidad de vecinos
    centrality = centrality_authority() # centralidad entre vecinos
  ) %>%
  # filter(!node_is_isolated()) %>%
  mutate(
    # group = group_infomap(trials = 500),
    group = group_leading_eigen(steps = 100),
    # group = group_louvain() # ok
    # group = group_optimal() # ok
    # group = group_walktrap(steps = 10) 
  ) %>%
  create_layout( layout = "gem") %>%
  ggraph::ggraph(layout) + 
    geom_edge_link(aes(width = correlation), alpha = 0.1) + 
    geom_node_point(aes(size = n, color = factor(group))) +
    geom_node_text(aes(label = name), size = 4, repel = TRUE) +
    theme_graph() +
    labs(title = "Keyword correlation to 'big data'",
         subtitle = "1 and 2 degree") 

rm(semnet,semgraph)
rm(corr_graph, correlaciones, keywords)

```

### Topic modelling sobre abstracts

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(quanteda)
library(stm)
library(tidytext)
library(textstem)

limpiar <- function(txt) {
  txt <- tolower(txt)
  txt <- gsub("[^a-z ]","",txt)
  txt <- gsub("big data","big_data",txt, fixed = TRUE)
  txt <- gsub("â"," ",txt, fixed = TRUE)
  txt <- trimws(txt)
  return(txt)
}


abstracts <- metadata %>% filter(tag=="abstract") %>%
  mutate(value=limpiar(value))

tidy_abs <- abstracts %>%
  select(-tag,-source) %>%
  unnest_tokens(output = word, input = value) %>%
  anti_join(stop_words) %>% # remove stopwords
  mutate(len = str_length(word)) %>% filter(len>2) %>% select(-len) %>% # remove 1-2 char words
  mutate(word=textstem::lemmatize_words(x = word %>% unlist()))  # lemmatize


dfm_abs <- tidy_abs %>%
  count(id, word, sort = TRUE) %>%
  tidytext::cast_dfm(id, word, n)

dfm_abs_idf <- tidy_abs %>%
  count(id, word, sort = TRUE) %>%
  tidytext::bind_tf_idf(tbl = ., document = id, term = word, n = n)

# 2do: tengo que bajar tf_idf a tm
# https://www.tidytextmining.com/dtm.html

# tm basico con stm + tidy -----------------
# https://juliasilge.com/blog/sherlock-holmes-stm/

topic_model <- stm(dfm_abs, K = 20, verbose = FALSE, 
                   init.type = "Spectral", max.em.its = 2)

td_beta <- tidy(topic_model)

td_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
    term = reorder_within(term, beta, topic)) 

td_gamma <- tidy(topic_model, matrix = "gamma", 
  document_names = rownames(dfm_abs))

library(ggthemes)

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(100, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest(cols=c(terms))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
    mutate(topic = paste0("Topic ", topic),
      topic = reorder(topic, gamma))

gamma_terms %>%
  #top_n(50, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005 ) +
  coord_flip() +
  #scale_y_continuous(expand = c(0,0),
  #  limits = c(0, 0.09)) +
  theme_tufte(ticks = FALSE) +
  theme(plot.title = element_text(size = 16 ),
    plot.subtitle = element_text(size = 13)) +
  labs(x = NULL, y = expression(gamma),
    title = "Top topics by prevalence")

gamma_terms %>%
  select(topic, gamma, terms) %>%
  knitr::kable(digits = 3, 
    col.names = c("Topic", "Expected topic proportion", "Top 7 terms"))

# # cuales son los +terminos de cada topico?
# gamma_terms %>%
#   select(topic, gamma, terms)
# 
# # cuales son los +docs de cada topico?
# docs_topico <- td_gamma %>%
#   group_by(topic) %>%
#   arrange(desc(gamma)) %>% 
#   top_n(100)
# docs_topico

# 2do: falta mismo preprocesamiento a ambas tablas
# cuales / cuantas palabras del tm estan en los keywords
# cuantos keywords aparecen sobre el total de palabras x topic?
# y cuan exclusivos son? ----> esta puede ser la nocion conceptual a explorar?

rm(x)

```

presentar como tabla de temas con topkeywords

## RQ3. Citation

### top venues

We identify the top 10 venues for publications in terms of number of citations. In addition, we also record the number of publication made in each venue. Lecture Notes in Computer Science (LNCS)2 received the most number of citations (3064), which published the most number of articles as well (2538). This resulted in 1.20 citations per publication. LNCS is followed by Proceedings of the VLDB Endowment which received 2369 citations for 99 publications. The top 15 venues based on absolute cita- tion counts are given in Table 5. The table also lists the number of publications and citations/publication for each venue. In terms of citations per publication, MIS Quar- terly: Management Information Systems ranks at the top. However, a close inspection of the data reveals that the MIS Quarterly: Management Information Systems has received 1111 citations for 5 publications. Out of 1111 citations, 1098 citations are for [13], whichmeans that the rest of4 publications received only 13 citations. Informa- tion Communication and Society ranks 2 based on citations/publication; however, one publication [9] accrued 999 citations, whereas the remaining 13 publications acquired 86 citations. Other top venues based on the citation/publication include Proceedings of the National Academy of Sciences of the United States ofAmerica (66.9), Nature (43.3), and IEEE Transactions on Knowledge and Data Engineering (31.78)

Table 5 Top venues by most number of citations Venue
x Citation count 
x Publication count 
x Citations/publication


lo que trajimos de anystyle

```{r message=FALSE, warning=FALSE}

table(references$CATEGORY)

```

armamos nuestra propia tabla

```{r}

ref <- references %>% 
  filter(CATEGORY %in% c("ARTICLE","BOOK","INPROCEEDINGS")) %>% 
  select(AUTHOR, JOURNAL, TITLE, DATE, DOI, BOOKTITLE,id) %>%
  rowwise() %>% # bajar autores a string
      mutate(authors = paste(AUTHOR, collapse=', ')) %>%
      ungroup() %>%
  mutate(
    date_anystyle = as.integer(DATE), # fecha parseada por anystyle
    date_titulo = as.integer(sub("\\D*(\\d{4}).*", "\\1", TITLE)), # fecha del titulo
  ) %>%
  mutate(date = case_when(
    is.na(date_anystyle) & !is.na(date_titulo) ~ date_titulo,
    is.na(date_titulo) & !is.na(date_anystyle) ~ date_anystyle,
    !is.na(date_anystyle) & !is.na(date_titulo) ~ date_anystyle,
    is.na(date_anystyle) & is.na(date_titulo) ~ NA_integer_
  )) %>% 
  mutate( ref = paste(
      str_replace_all(string = authors, pattern = "[^[A-Za-z ,]]", replacement = ""),
      date,
      str_replace_all(string = TITLE, pattern = "[^[A-Za-z ]]", replacement = "") %>%
        str_sub(string = ., 1, 20)
      )
  ) %>%
  mutate( ref = trimws(tolower(ref)))

glimpse(ref)

# 2do: falta limpiar y borrar los na obvios

```
cantidad de referencias??? 

```{r}
ref %>% count(ref) %>% count(n, sort = TRUE) %>% plot()
```

### citas x año x articulos

What is the average number of citations/publications?

```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
  left_join(
    metadata %>% # citas por año
    filter(tag=="year") %>%
    select(id,year=value)) %>% 
    mutate(year=as.integer(year)
  ) %>%
  count(year, name = 'references') %>%
  ungroup() %>%
  left_join(
    metadata %>% # articulos por año
    filter(tag=="year") %>%
    select(id,year=value) %>% 
    mutate(year=as.integer(year)) %>%
             count(year, name = 'articulos')
  ) %>%
  mutate(
    ref_art = references / articulos
  )

```


### Articulos más influyentes (+citas)


```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
  count(ref,sort = TRUE) %>% head()

```

#### publications in the selected time frame

**2do: calcular**

```{r echo=FALSE, message=FALSE, warning=FALSE}


```


#### Top cited publications for each year

```{r echo=FALSE, message=FALSE, warning=FALSE}

ref %>% 
    left_join(
      metadata %>% 
      filter(tag=="year") %>%
      select(id,year=value)) %>% 
      mutate(year=as.integer(year)
    ) %>%
    group_by(year,ref) %>%
    summarise(n=n()) %>%
    filter(n>1) %>%
    slice_max(order_by = n, n = 5, with_ties = FALSE) %>%
  ggplot(aes(y=n,x=ref)) + geom_col() + facet_wrap(~year) + coord_flip()

```

#### Top 10 publications by normalized score

**2do: calcular**


```{r echo=FALSE, message=FALSE, warning=FALSE}

```


#### referencias por topicos (ya sea x tm o keywords)

**2do: calcular**


```{r echo=FALSE, message=FALSE, warning=FALSE}

```

# Discussion

**meter comparacion con el general**

Discussions and suggestions for future research This study presented a comprehensive review of the current studies related to big data. Based on a sample size of 4,070 articles obtained from the Scopus database published between 2013 and 2018, this study revealed an array of interesting findings by utilizing the bibliometric approach and SciVal metrics. The results have provided a comprehensive understanding of big data research, in particular, in view of the scope, co-production and performance of this research stream.

This paper delivers a comprehensive review of research in BDA to foster a better understanding of a pervasive and new area. Several fundamental research questions that are relevant to current research efforts are focusing on the BDAA have been answered while providing a better platform for further evaluation and research in this area. A robust methodology called “Systematic Network Analysis of Literature” has been adopted for this purpose. This study conducted an SLR of 79 published papers and bibliometric analysis of 516 published papers from various journals, from 2014 to 2018. The research papers included in the review were selected based on the title, introduction, abstract, conclusion and research methods. A bibliometric analysis includes an integrated and systematic review of the literature to explore key themes and the theoretical framework underpinning journals, the field of inquiry or papers (Garc?ıa-Lillo et al., 2018). Due to their benefits over other approaches

## Limitations and future scope 

While this review paper has been completed rigorously, some limitations may present suggestions for future review papers. The data were collected from different journals, whereas the document examined did not include unpublished papers, master or doctoral theses and textbooks regarding BDAA. While this study has attempted to provide a comprehensive review based on the current literature, the data can be collected from the resources mentioned above as a recommendation for future studies, and the data reported and obtained in this study can be compared with the results obtained from those resources. This paper reviewed classified papers in the seven categories; it is suggested that future papers can classify and review papers in different areas and subcategories. Another limitation of this review was the extraction of all papers from English-language journals only. Consequently, the review did not include scientific journals in other languages. This study,
however, believes that most of the reviewed papers were from international journals. This study carefully selected and summarized the papers available in the Web of Science databases from several publishers. However, certain relevant outlets remained outside the scope of the current study. Therefore, future researchers may try to review papers not considered in the current review article. Another limitation is that in order to address the second research question, BDAA in different countries, the bibliometric analysis was carried out with only specific keywords to select Webof Science database papers; future researchmay aim at using different keywords and objectives to obtain different results. In the future effect of BDA on supply chain risk and flexibility can be studied. Role of artificial intelligence (AI) on BDA can be further analyze
It is important to present the limitation of the study, as it might be possible that the results are not obtained if the same set of experiments is repeated again. We downloaded the data from Scopus on January 27, 2018. As Scopus has data download limits, the data were downloaded in an incremental manner and later combined. It is possible that the reader may find the number of citations for various articles different than the ones reported here. There can be several reasons for this. For example, the article might have accumulated more citations over time. It is also possible that various sources might report different statistics for the same article. A noticeable example is the case of publication“Data mining with big data.” The paper according to Scopus has received 681 citations, whereas Google Scholar has recorded 1320 citations for the publication till 2017. The publisher IEEE Transactions on Knowledge and Data Engineering recorded 524 citations for the same publication. The difference between the citation count of Scopus and other sources can be attributed to the fact that Scopus citations are based on Scopus-indexed publications only. It is also important to mention that Google Scholar citations also include non-academic citations. As stated earlier that a variety of past studies has validated the authenticity of Scopus, therefore, our results are also based on the statistics of Scopus only

# Referencias

